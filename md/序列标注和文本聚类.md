## 序列标注和文本聚类

### 1. 序列标注

$$
\hat y = \mathop{argmax}_yf(\phi(x,y),\theta)  \hspace{8cm} (1)
$$

x为句子序列，y为标注序列

argmax 对应解码过程，$y$若是序列，需要使用动态规划算法，$y$若是分类，否则使用穷举法即可。

#### 中文分词简介

#### 1.1 BMES 编码

单个字作为词：S

双字作为词：B（起始），E（结束）

多字作为词：B（起始），M（中间字），E（结束）

例子：他（S）说（S）的（S）确（B）实（E）在（B）理（E）

#### 1.2 特征抽取

$y$序列之间存在依赖关系（如$y_{i-1}=B$和$y_i=B$这种组合是不会出现的）

设计如下模板来抽取特征：

1. $[y_{-1},y_{0}]$ 抽取$y$序列之间的特征
2. $[x_{-1},x_{0},y_{0}]$抽取(x,y)之间的特征

模板1抽出的特征例子如下：
$$
\phi_k(x,y_{i})=\left\{
\begin{array}{11}
1 &  if\ y_{i-1}=B,\ y_{i}=E\\
0 & \textrm{otherwise} 
\end{array}
\right..
$$
模板2抽出的特征例子如下：
$$
\phi_n(x,y_i) =\left\{
\begin{array}{11}
1 &  if\ x_{i}\ =\ 他\ and\ x_{i-1}\ =\ <s>\ and \ y_{i}=S\\
0 & \textrm{otherwise} 
\end{array}
\right.
$$

#### 1.3 优化方法

若将$y$序列代入式（1），则$y$的解空间共有$4^L$大小（L为序列长度），无法穷举获得最佳解

简化步骤：
$$
\theta^{T}\phi(x,y) \approx \sum_{i=1}^{L}\theta^{T}\phi(x,y_{i-1},y_{i})\approx\sum_{i=1}^{L}\theta_{1}^{T}\phi_{1}(x,y_i)+\theta_{2}^{T}\phi_{2}(y_{i-1},y_i)\hspace{3cm}(2)
$$
特征提取模板与$\phi$空间维数的关系：

$[x_0,y_0]\to\phi_1\in R^{|V|*4}$

$[y_{-1},y_{0}]\to\phi_{2}\in R^{16}$

$[x_{-1},x_{0},y_{0}]\to\phi_{1}\in R^{|V|*4*4}$

问题转化为：
$$
\hat{y_{1}}...\hat {y_L} = \mathop {argmax}_{y_1...y_L} \sum_{i=1}^{L}\theta_{1}^{T}\phi_{1}(x,y_i)+\theta_{2}^{T}\phi_{2}(y_{i-1},y_i)
$$

#### 1.4  预测y序列

$max(a+b,a+c) = a + max(b+c)$
$$
\mathop {argmax}_{y_1...y_L} \sum_{i=1}^{L}\theta_{1}^{T}\phi_{1}(x,y_i)+\theta_{2}^{T}\phi_{2}(y_{i-1},y_i) = \mathop {argmax}_{y_1} \mathop {argmax}_{y_2}\cdots\mathop {argmax}_{y_L} \sum_{i=1}^{L}\theta_{1}^{T}\phi_{1}(x,y_i)+\theta_{2}^{T}\phi_{2}(y_{i-1},y_i)\ \ \  (2)
$$
抽取只和$y_1$相关的特征，（2）式又可写成
$$
\mathop {argmax}_{y_1}(\theta_{1}^{T}\phi_{1}(x,y_1)+\theta_{2}^{T}\phi_{2}(y_0,y_1)+\theta_{2}^{T}\phi_{2}(y_1,y_2))+\mathop {argmax}_{y_2}(\cdots)+\cdots
$$
运算次数由$4^L$降低为$4L$

viterbi算法：

定义$\alpha_{t,i}:t\in{B,M,E,S}$
$$
\left(
		\begin{array} {ccc} \alpha_{t,0}=0\\
		\alpha_{t,i} = \mathop {max}_{t^{'}}(\alpha_{t,i-1}+\theta_1^{T}\phi_{1}(x,t^{'})+\theta_2^{T}\phi_2(t^{'},t))
		\end{array}
	      \right.
$$
![](http://ogfba2fhh.bkt.clouddn.com/Viterbi_Crf.png)

记录路径，得到最佳序列。

### 2.文本聚类

文本聚类方法可以分为静态聚类和动态聚类，静态聚类方法包括Top-down方法和Bottom-up方法，动态聚类（Online clustering）需要判断每个新加入的样本属于已有的类还是一个新类。

#### 2.1 K-mean聚类（Top-down）

K-means聚类前提：样本之间相似度能够计算

1. 随机在图中取$K$个种子点。
2. 然后对图中的所有点求到这$K$个种子点的距离，假如点$P_i$离种子点$S_i$最近，那么$P_i$属于$S_i$点群。
3. 接下来，我们要移动种子点到属于他的“点群”的中心。
4. 然后重复第2）和第3）步，直到种子点没有移动。

K-means缺点：

1. 需提前确定聚类数目
2. 对初始选取的点很敏感
3. 属于硬聚类（每个样本只能属于一类）

#### 2.2 布朗聚类（Bottom-Up）

##### 2.2.1 预备知识

**熵**

若$X$是一个离散型随机变量，取值空间为$R$，其概率分布为$p(x)=P(X=x),x\in R$。那么，$X$的熵$H(X)$定义为
$$
H(X) ＝ -\sum_{x\in R}p(x)log_2p(x)
$$
其中约定$0log_20 ＝ 0$，通常将$log_2p(x)$写作$logp(x)$

熵又称为自信息，可以视为描述一个随机变量的不确定性的数量。

**联合熵和条件熵**

若$X,Y$是一对离散型随机变量$X,Y ～ p(x,y)$，则$X,Y$的联合熵$H(X,Y)$
$$
H(X,Y) = -\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(x,y)
$$
联合熵实际就是描述一对随机变量平均所需的信息量

给定随机变量$X$的情况下，随机变量$Y$的条件熵如下：
$$
H(Y|X) = -\sum_{x\in X}\sum_{y\in Y}p(x,y)logp(y|x)
$$
条件熵和联合熵的关系：
$$
H(X,Y)  ＝ H(Y|X) + H(X)
$$
推广到一般情况：
$$
H(X_1,X_2,...,X_n) = H(X_1)+H(X_2|X_1)+...+H(X_n|X_1,...,X_{n-1})
$$
互信息
$$
H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)
$$
可得

$$
H(X)-H(X|Y) = H(Y)-H(Y|X)
$$
这个差称为$X$和$Y$ 的**互信息**，记作$I(X;Y)$，可得$I(X;Y)=H(X)-H(X|Y)$

$I(X;Y)$反映的是在知道了$Y$的值以后$X$的不确定性的减少量。

 ![mutualInfo](http://ogfba2fhh.bkt.clouddn.com/mutualInfo.png)
$$
I(X;Y) = \sum_{x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)}
$$

##### 2.2.2 布朗聚类：

布朗聚类是一种自底而上的层次聚类算法，基于n-gram模型和马尔科夫链模型，是一种硬聚类，每个词都在且只在唯一的一个类中。

![](http://ogfba2fhh.bkt.clouddn.com/brown_clustering.png)

$w$是词，$c$是类，不同于词性标注，此处$c$是未知的。

布朗聚类的输入是一个语料库，这个语料库是一个词序列，输出是一个二叉树，树的叶子节点是一个个词，树的中间节点是我们想要的类（中间结点作为根节点的子树上的所有叶子为类中的词）。
$$
Quality(C)=\frac{1}{n}logP(w_1,\cdots,w_n) =\frac{1}{n}logP(w_1,\cdots,w_n,C(w_1),\cdots,C(w_n))\\
=\frac{1}{n}log\prod_{i=1}^{n}P(C(w_i)|C(w_{i-1}))P(w_i|C(w_i))\hspace{1.8cm}
$$
$C(w_0)$是种特殊的‘Start’类，我们要找到这样的分类方式$C$，使得$Quality(C)$尽可能大
$$
Quality(C) =\frac{1}{n}\sum_{i=1}^{n}P(C(w_i)|C(w_{i-1}))P(w_i|C(w_i))\hspace{3.7cm}\\=\sum_{w^{'},w}\frac{n(w,w^{'})}{n}logP(C(w^{'})|C(w)))P(w^{'}|C(w^{'}))\\=\sum_{c,c{'}}\frac{n(c,c^{'})}{n}log\frac{n(c,c{'})}{n(c)n(c')}+\sum_{w^{'}} \frac {n(w^{'})}{n}log\frac{n(w')}{n}
$$
$n(w)$是$w$在文本中出现次数，$n(w,w^{'})$是二元对$w,w^{'}$在文本中的出现次数，$n(c)=\sum_{w\in c}n(w)$，$n(c,c^{'})=\sum_{w\in c}n(w,w^{'})$		
$$
Quality(C) = I(C)-H
$$
**简单算法:**

以$k$长度文本为例：每个字均为一类，词典大小为$|V|$，找到俩类，合并后使得互信息变得最大，重复合并步骤使得最后都归为一类，整个算法的时间复杂度为$O(|V|^{5})$

**优化算法：**

开始设置一个参数m，比如$m=1000$，我们按照词汇出现的频率对其进行排序

 然后把频率最高的$m$个词各自分到一个类中，对于第$m+1$到$|V|$个词进行循环：

1.对当前词新建一个类，我们现在又$m+1$个类了

2.从这$m+1$个类中贪心选择最好的两个类合并，现在我们剩下$m$个类

 最后我们再做$m-1$词合并。算法时间复杂度$O(|V|*m^2)$

